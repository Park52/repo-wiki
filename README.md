# RepoWiki

**Agentic AI CLI for repository Q&A and wiki generation**

RepoWikiëŠ” LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì½”ë“œë² ì´ìŠ¤ë¥¼ ë¶„ì„í•˜ê³  ì§ˆë¬¸ì— ë‹µë³€í•˜ë©°, ìë™ìœ¼ë¡œ ìœ„í‚¤ ë¬¸ì„œë¥¼ ìƒì„±í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## âœ¨ Features

- ğŸ¤– **Agentic AI**: LLMì´ tool-callingì„ í†µí•´ ììœ¨ì ìœ¼ë¡œ ì½”ë“œ íƒìƒ‰
- ğŸ” **Code Search**: SQLite FTS5 ê¸°ë°˜ ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰
- ğŸ“š **Auto Wiki**: ë ˆí¬ì§€í† ë¦¬ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ìœ„í‚¤ í˜ì´ì§€ ìë™ ìƒì„±
- ğŸ¯ **Source Verification**: ëª¨ë“  ë‹µë³€ì— ê·¼ê±°(íŒŒì¼ ê²½ë¡œ + ë¼ì¸ ë²ˆí˜¸) í¬í•¨
- ğŸ› ï¸ **Extensible Tools**: ê²€ìƒ‰, íŒŒì¼ ì½ê¸°, ê·¸ë˜í”„ íƒìƒ‰ ë“± ë‹¤ì–‘í•œ ë„êµ¬
- ğŸ”Œ **Multi-Provider**: OpenAI, Anthropic, Ollama, LM Studio, Together, Groq ë“± ì§€ì›

---

## ğŸ“¦ Installation

```bash
# Clone repository
git clone https://github.com/Park52/repo-wiki.git
cd repo-wiki

# Install dependencies
npm install

# Build packages
npm run build

# Link CLI globally (optional)
npm link --workspace=@repo-wiki/cli
```

---

## ğŸš€ Quick Start

### 1. Set API Key (for cloud providers)

```bash
# OpenAI
export OPENAI_API_KEY=sk-...

# Or Anthropic
export ANTHROPIC_API_KEY=sk-ant-...

# Or Together AI
export TOGETHER_API_KEY=...

# Or Groq
export GROQ_API_KEY=...
```

### 2. Ask Questions

```bash
# Ask about your codebase (default: OpenAI gpt-4o)
repowiki ask "What is the main architecture of this project?"

# Use Claude
repowiki ask --provider anthropic "Explain the agent loop"

# Use local Ollama (no API key needed!)
repowiki ask --provider ollama --model llama3.1 "What tools are available?"

# Use LM Studio (local)
repowiki ask --provider lmstudio --base-url http://localhost:1234/v1 "Summarize this project"

# Save answer and steps
repowiki ask "How does the agent loop work?" \
  --out answer.md \
  --steps steps.json \
  --verbose
```

### 3. Generate Wiki

```bash
# Generate specific wiki page
repowiki wiki --page overview --out ./wiki

# Generate all wiki pages
repowiki wiki --all --out ./wiki

# Available page types:
# - overview: Project overview and purpose
# - build: Build & setup instructions
# - architecture: Architecture and components
# - modules: Module descriptions
```

---

## ğŸ“– Commands

### `repowiki ask`

ë ˆí¬ì§€í† ë¦¬ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.

```bash
repowiki ask "<question>" [options]
```

**Options:**
- `-r, --repo <path>` - Repository path (default: current directory)
- `-p, --provider <name>` - LLM provider: openai, anthropic, ollama, lmstudio, together, groq (default: openai)
- `-m, --model <model>` - LLM model (auto-selected per provider if omitted)
- `-u, --base-url <url>` - Base URL for local/custom providers
- `-s, --max-steps <n>` - Maximum agent steps (default: 8)
- `-o, --out <path>` - Save answer markdown to file
- `--steps <path>` - Save execution steps to JSON
- `-v, --verbose` - Show detailed step logs

**Examples:**
```bash
# Basic question (OpenAI)
repowiki ask "Explain the AgentLoop implementation"

# Use Claude (Anthropic)
repowiki ask --provider anthropic "Explain the codebase structure"

# Use local Ollama
repowiki ask --provider ollama --model mistral "What does this project do?"

# Use Groq (fast inference)
repowiki ask --provider groq --model llama-3.1-70b-versatile "Summarize the architecture"

# Save outputs
repowiki ask "What tools are available?" --out answer.md --steps debug.json

# Custom model and steps
repowiki ask "How does indexing work?" --model gpt-4-turbo --max-steps 10
```

### `repowiki wiki`

ìœ„í‚¤ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```bash
repowiki wiki [options]
```

**Options:**
- `-r, --repo <path>` - Repository path (default: current directory)
- `-p, --provider <name>` - LLM provider: openai, anthropic, ollama, lmstudio, together, groq (default: openai)
- `-m, --model <model>` - LLM model (auto-selected per provider if omitted)
- `-u, --base-url <url>` - Base URL for local/custom providers
- `--page <type>` - Page type: overview|build|architecture|modules
- `-a, --all` - Generate all wiki pages
- `-o, --out <dir>` - Output directory (default: ./wiki)
- `-v, --verbose` - Show detailed progress

**Examples:**
```bash
# Single page (OpenAI)
repowiki wiki --page overview

# All pages with Claude
repowiki wiki --all --provider anthropic --out ./docs/wiki

# Using local Ollama
repowiki wiki --provider ollama --model llama3.1 --all

# Custom output
repowiki wiki --page architecture --out ./architecture.md
```

### `repowiki index`

ë ˆí¬ì§€í† ë¦¬ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```bash
repowiki index [options]
```

**Options:**
- `-r, --repo <path>` - Repository path

**Note:** `ask`ì™€ `wiki` ëª…ë ¹ì–´ëŠ” ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

---

## ğŸ—ï¸ Architecture

```
repo-wiki/
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ core/              # Core library
â”‚   â”‚   â”œâ”€â”€ agent/         # AgentLoop, verifier
â”‚   â”‚   â”œâ”€â”€ llm/           # LLM providers (OpenAI, Local)
â”‚   â”‚   â”œâ”€â”€ tools/         # Tool registry & implementations
â”‚   â”‚   â””â”€â”€ indexer/       # SQLite FTS5 indexer
â”‚   â””â”€â”€ cli/               # CLI commands
â”‚       â”œâ”€â”€ commands/      # ask, wiki, index
â”‚       â””â”€â”€ utils.ts       # Shared utilities
```

### Agent Loop Flow

```
1. User Question
   â†“
2. System Prompt + Tools â†’ LLM
   â†“
3. LLM Response:
   â”œâ”€ Tool Calls â†’ Execute â†’ Add to Context â†’ Loop to 2
   â””â”€ DONE + Answer â†’ Verify Sources â†’ Return
```

### Available Tools

| Tool | Description |
|------|-------------|
| `search_chunks` | Search indexed code chunks (FTS5) |
| `get_excerpt` | Read file excerpt by line range |
| `graph_neighbors` | Find related code (imports/exports) |
| `list_files` | List files matching glob pattern |
| `get_repo_summary` | Get repository overview |

---

## ğŸ”§ Configuration

### Supported Providers

| Provider | Type | API Key Env Var | Default Model |
|----------|------|-----------------|---------------|
| `openai` | Cloud | `OPENAI_API_KEY` | gpt-4o |
| `anthropic` | Cloud | `ANTHROPIC_API_KEY` | claude-3-5-sonnet-latest |
| `together` | Cloud | `TOGETHER_API_KEY` | meta-llama/Llama-3-70b-chat-hf |
| `groq` | Cloud | `GROQ_API_KEY` | llama-3.1-70b-versatile |
| `ollama` | Local | (none) | llama3.1 |
| `lmstudio` | Local | (none) | local-model |

### Environment Variables

```bash
# Cloud Providers (pick one)
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
TOGETHER_API_KEY=...
GROQ_API_KEY=...

# Optional
OPENAI_BASE_URL=https://api.openai.com/v1  # Custom endpoint
```

### Local LLM Setup

#### Ollama (Recommended for local)
```bash
# Install Ollama: https://ollama.ai
ollama pull llama3.1
ollama serve  # Start server

# Use with RepoWiki
repowiki ask --provider ollama "What is this project?"
```

#### LM Studio
```bash
# Download LM Studio: https://lmstudio.ai
# Load a model and start the local server (port 1234)

# Use with RepoWiki
repowiki ask --provider lmstudio --base-url http://localhost:1234/v1 "Describe the architecture"
```

### Custom LLM Providers

```typescript
import { runAgent, LlmProvider } from '@repo-wiki/core';

class MyProvider implements LlmProvider {
  // Implement chat() method
}

const result = await runAgent({
  repoRoot: '/path/to/repo',
  question: 'How does X work?',
  llmProvider: new MyProvider(),
});
```

## ğŸ§ª ë¡œì»¬ LLMìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ê¸°

### ìë™í™”ëœ Ollama í…ŒìŠ¤íŠ¸

RepoWikië¥¼ Ollamaë¡œ ê²€ì¦í•˜ëŠ” 3ê°œì˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

```bash
# 1. ìµœì´ˆ ì„¤ì • + ì¢…í•© í…ŒìŠ¤íŠ¸ (ì²˜ìŒ ì‹¤í–‰ ì‹œ)
./scripts/setup-ollama.sh && ./scripts/test-with-ollama.sh

# 2. ë¹ ë¥¸ ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ (Ollama ì´ë¯¸ ì„¤ì¹˜ëœ ê²½ìš°)
./scripts/quick-test.sh

# 3. ì „ì²´ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ (5ê°œ í…ŒìŠ¤íŠ¸: ì¸ë±ìŠ¤, ì§ˆë¬¸, ìœ„í‚¤, verbose, ì„±ëŠ¥)
./scripts/test-with-ollama.sh
```

**ê° ìŠ¤í¬ë¦½íŠ¸ ì„¤ëª…:**
- `setup-ollama.sh`: Ollama ì„¤ì¹˜ (macOS/Linux), llama3.1 ë‹¤ìš´ë¡œë“œ, ì„œë²„ ì‹œì‘
- `test-with-ollama.sh`: ëª¨ë“  ì£¼ìš” ê¸°ëŠ¥ì„ ì»¤ë²„í•˜ëŠ” ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- `quick-test.sh`: ë¹ ë¥¸ ê²€ì¦ì„ ìœ„í•œ ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸

---

## ğŸ“Š Output Format

### Answer Markdown

```markdown
The AgentLoop orchestrates LLM interactions by...

## Sources
- `packages/core/src/agent/loop.ts`:15-45
- `packages/core/src/types.ts`:10-25
```

### Steps JSON

```json
{
  "question": "How does the agent loop work?",
  "totalMs": 3542,
  "verified": true,
  "steps": [
    {
      "stepNo": 1,
      "toolName": "search_chunks",
      "elapsedMs": 234,
      "isDone": false
    }
  ],
  "sources": [...]
}
```

---

## ğŸ› ï¸ Development

```bash
# Install dependencies
npm install

# Build all packages
npm run build

# Watch mode
npm run dev

# Type check
npm run typecheck

# Clean build artifacts
npm run clean
```

### Package Structure

- `@repo-wiki/core` - Core library (agent, LLM, tools, indexer)
- `@repo-wiki/cli` - CLI application

---

## ğŸ“ How It Works

### 1. Indexing
SQLite FTS5ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  íŒŒì¼ì„ ì¸ë±ìŠ¤í™”í•©ë‹ˆë‹¤:
```typescript
const indexer = new Indexer({ repoPath: '/path/to/repo' });
await indexer.indexRepository();
```

### 2. Agent Loop
LLMì´ tool-callingì„ í†µí•´ ììœ¨ì ìœ¼ë¡œ ì½”ë“œë¥¼ íƒìƒ‰:
1. LLMì—ê²Œ ì§ˆë¬¸ + ì‚¬ìš© ê°€ëŠ¥í•œ tools ì œê³µ
2. LLMì´ tool call ë˜ëŠ” ìµœì¢… ë‹µë³€ ë°˜í™˜
3. Tool ì‹¤í–‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
4. ìµœëŒ€ 8 stepê¹Œì§€ ë°˜ë³µ
5. Sources ê²€ì¦ í›„ ë‹µë³€ ë°˜í™˜

### 3. Source Verification
ëª¨ë“  ë‹µë³€ì€ ë°˜ë“œì‹œ Sources ì„¹ì…˜ì„ í¬í•¨í•´ì•¼ í•˜ë©°:
- Format: `` `path/to/file.ts`:startLine-endLine ``
- íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ë° ë¼ì¸ ë²”ìœ„ ê²€ì¦
- ê²€ì¦ ì‹¤íŒ¨ ì‹œ LLMì—ê²Œ ì¬ì‹œë„ ìš”ì²­

---

## âš ï¸ Current Limitations

- **LLM Provider**: OpenAIë§Œ ì™„ì „ ì§€ì› (LocalProviderëŠ” stub)
- **Graph Analysis**: Import/export ë¶„ì„ì´ ê¸°ë³¸ì  ìˆ˜ì¤€
- **Language Support**: ì£¼ìš” ì–¸ì–´ë§Œ ì¸ë±ì‹± (.ts, .js, .py, .rs, .go, .java ë“±)
- **Max Steps**: ìµœëŒ€ 8 step ì œí•œ (ë³µì¡í•œ ì§ˆë¬¸ì€ ë‹µë³€ ë¶ˆê°€ëŠ¥í•  ìˆ˜ ìˆìŒ)

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

Built with:
- [OpenAI](https://openai.com/) - LLM API
- [better-sqlite3](https://github.com/WiseLibs/better-sqlite3) - SQLite with FTS5
- [Commander.js](https://github.com/tj/commander.js) - CLI framework
- [Zod](https://github.com/colinhacks/zod) - Schema validation

---

**Status**: âœ… **Production Ready** - Core features implemented and tested